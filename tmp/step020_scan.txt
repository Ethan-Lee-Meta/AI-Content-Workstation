== [A] snapshot ==
root=E:/01Small_Tools/AI_Content_Generation_Workstation/ai-content-workstation
branch=dev/v1_1-batch1-step020-data_model
head=e26bf410fe890a843a40a6bad8897a922b71ca22

== [B] alembic head(s) ==
0003_optional_hierarchy (head)

== [C] alembic env.py (top 160 lines) ==
from __future__ import annotations

import os
import sys
from pathlib import Path
from logging.config import fileConfig

from sqlalchemy import engine_from_config, pool
from alembic import context

THIS = Path(__file__).resolve()
API_DIR = THIS.parents[1]  # apps/api
sys.path.insert(0, str(API_DIR))

config = context.config
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# STEP-030: bootstrap only (no metadata yet). STEP-040 will set target_metadata.
target_metadata = None


def _repo_root() -> Path:
    # apps/api/migrations/env.py -> repo root = parents[3]
    return THIS.parents[3]


def _resolve_sqlite_url(url: str) -> str:
    if not url.startswith("sqlite:///"):
        return url
    p = url[len("sqlite:///") :]

    # absolute unix
    if p.startswith("/"):
        return url

    # absolute windows drive
    if len(p) >= 3 and p[1] == ":" and (p[2] == "/" or p[2] == "\\"):
        return url

    abs_path = (_repo_root() / p).resolve()
    abs_path.parent.mkdir(parents=True, exist_ok=True)
    return "sqlite:///" + abs_path.as_posix()


def get_url() -> str:
    url = os.getenv("DATABASE_URL", "sqlite:///./data/app.db")
    return _resolve_sqlite_url(url)


def run_migrations_offline() -> None:
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )
    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    configuration = config.get_section(config.config_ini_section) or {}
    configuration["sqlalchemy.url"] = get_url()

    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
        future=True,
    )

    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)
        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

== [D] db wiring candidates (files) ==
- apps/api/app/core/db.py

== [E] id strategy signals (top 30 matches) ==
apps/api/app/main.py:118:    rid = request.headers.get("X-Request-Id") or uuid.uuid4().hex.upper()
apps/api/app/main.py.bak.batch9.20251231_140807:116:    rid = request.headers.get("X-Request-Id") or uuid.uuid4().hex.upper()
apps/api/app/modules/exports_imports/service.py:31:    return uuid.uuid4().hex.upper()
apps/api/app/modules/runs/service.py:91:        # if PK is integer, let DB autoincrement; otherwise generate ULID
apps/api/app/modules/shots/router.py:43:    return uuid.uuid4().hex.upper()
apps/api/app/modules/shots/router.py:329:        link_id = uuid.uuid4().hex.upper()
apps/api/app/modules/shots/router.py:408:        tombstone_id = uuid.uuid4().hex.upper()
apps/api/app/modules/shots/schemas.py:31:    assets: List[Dict[str, str]] = Field(default_factory=list)        # [{"asset_id": "..."}]
apps/api/app/modules/shots/schemas.py:32:    runs: List[Dict[str, str]] = Field(default_factory=list)          # [{"run_id": "..."}]
apps/api/app/modules/shots/schemas.py:33:    prompt_packs: List[Dict[str, str]] = Field(default_factory=list)  # [{"prompt_pack_id": "..."}]
apps/api/app/modules/shots/schemas.py:34:    series: List[Dict[str, str]] = Field(default_factory=list)        # [{"series_id": "..."}]
apps/api/app/modules/shots/schemas.py:35:    projects: List[Dict[str, str]] = Field(default_factory=list)      # [{"project_id": "..."}]
apps/api/app/modules/shots/schemas.py:36:    other: List[Dict[str, str]] = Field(default_factory=list)         # [{"dst_type": "...", "dst_id": "..."}]

== [F] timestamp strategy signals (top 30 matches) ==
apps/api/app/main.py:87:    return datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat()
apps/api/app/main.py.bak.batch9.20251231_140807:85:    return datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat()
apps/api/app/modules/assets/schemas.py:18:    created_at: Optional[str] = None
apps/api/app/modules/assets/service.py:50:    if "created_at" in cols:
apps/api/app/modules/assets/service.py:51:        return "created_at DESC"
apps/api/app/modules/assets/service.py:62:        "created_at",
apps/api/app/modules/assets/service.py:183:            for k in ["id", "created_at"]:
apps/api/app/modules/assets/service.py:254:            ts = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
apps/api/app/modules/exports_imports/schemas.py:20:    created_at: str
apps/api/app/modules/exports_imports/schemas.py:35:    created_at: str
apps/api/app/modules/exports_imports/schemas.py:58:    created_at: str
apps/api/app/modules/exports_imports/service.py:27:    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
apps/api/app/modules/exports_imports/service.py:56:        return v.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")
apps/api/app/modules/exports_imports/service.py:213:    created_at = _now_iso()
apps/api/app/modules/exports_imports/service.py:315:        "created_at": created_at,
apps/api/app/modules/exports_imports/service.py:336:        "created_at": created_at,
apps/api/app/modules/exports_imports/service.py:398:    created_at = _now_iso()
apps/api/app/modules/exports_imports/service.py:543:        "created_at": created_at,
apps/api/app/modules/reviews/service.py:30:    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")
apps/api/app/modules/reviews/service.py:93:    for k in ("created_at", "updated_at", "submitted_at"):
apps/api/app/modules/runs/router.py:107:        created_at=row["created_at"],
apps/api/app/modules/runs/schemas.py:35:    created_at: Optional[str] = None
apps/api/app/modules/runs/service.py:31:    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")
apps/api/app/modules/runs/service.py:96:    for k in ("created_at", "updated_at", "submitted_at"):
apps/api/app/modules/runs/service.py:242:        if "updated_at" in cols:
apps/api/app/modules/runs/service.py:243:            updates["updated_at"] = _now_iso()
apps/api/app/modules/runs/service.py:278:        created_at = None
apps/api/app/modules/runs/service.py:279:        for k in ("created_at", "submitted_at"):
apps/api/app/modules/runs/service.py:281:                created_at = row[k]
apps/api/app/modules/runs/service.py:308:            "created_at": str(created_at) if created_at is not None else None,

== [G] existing append-only trigger style (migrations) ==
apps/api/migrations/versions/0002_core_entities.py:1:"""core entities v0 (Asset/PromptPack/Run/Review/Link) + append-only triggers
apps/api/migrations/versions/0002_core_entities.py:84:    # ---- append-only invariants (SQLite triggers) ----
apps/api/migrations/versions/0002_core_entities.py:85:    # Policy: PromptPack / Run / Review append-only (no UPDATE / no DELETE)
apps/api/migrations/versions/0002_core_entities.py:88:    CREATE TRIGGER IF NOT EXISTS trg_prompt_packs_no_update
apps/api/migrations/versions/0002_core_entities.py:89:    BEFORE UPDATE ON prompt_packs
apps/api/migrations/versions/0002_core_entities.py:91:      SELECT RAISE(ABORT, 'append-only: prompt_packs cannot be updated');
apps/api/migrations/versions/0002_core_entities.py:95:    CREATE TRIGGER IF NOT EXISTS trg_prompt_packs_no_delete
apps/api/migrations/versions/0002_core_entities.py:96:    BEFORE DELETE ON prompt_packs
apps/api/migrations/versions/0002_core_entities.py:98:      SELECT RAISE(ABORT, 'append-only: prompt_packs cannot be deleted');
apps/api/migrations/versions/0002_core_entities.py:103:    CREATE TRIGGER IF NOT EXISTS trg_runs_no_update
apps/api/migrations/versions/0002_core_entities.py:104:    BEFORE UPDATE ON runs
apps/api/migrations/versions/0002_core_entities.py:106:      SELECT RAISE(ABORT, 'append-only: runs cannot be updated');
apps/api/migrations/versions/0002_core_entities.py:110:    CREATE TRIGGER IF NOT EXISTS trg_runs_no_delete
apps/api/migrations/versions/0002_core_entities.py:111:    BEFORE DELETE ON runs
apps/api/migrations/versions/0002_core_entities.py:113:      SELECT RAISE(ABORT, 'append-only: runs cannot be deleted');
apps/api/migrations/versions/0002_core_entities.py:118:    CREATE TRIGGER IF NOT EXISTS trg_reviews_no_update
apps/api/migrations/versions/0002_core_entities.py:119:    BEFORE UPDATE ON reviews
apps/api/migrations/versions/0002_core_entities.py:121:      SELECT RAISE(ABORT, 'append-only: reviews cannot be updated');
apps/api/migrations/versions/0002_core_entities.py:125:    CREATE TRIGGER IF NOT EXISTS trg_reviews_no_delete
apps/api/migrations/versions/0002_core_entities.py:126:    BEFORE DELETE ON reviews
apps/api/migrations/versions/0002_core_entities.py:128:      SELECT RAISE(ABORT, 'append-only: reviews cannot be deleted');
apps/api/migrations/versions/0002_core_entities.py:133:    CREATE TRIGGER IF NOT EXISTS trg_links_no_update
apps/api/migrations/versions/0002_core_entities.py:134:    BEFORE UPDATE ON links
apps/api/migrations/versions/0002_core_entities.py:136:      SELECT RAISE(ABORT, 'append-only: links cannot be updated');
apps/api/migrations/versions/0002_core_entities.py:140:    CREATE TRIGGER IF NOT EXISTS trg_links_no_delete
apps/api/migrations/versions/0002_core_entities.py:141:    BEFORE DELETE ON links
apps/api/migrations/versions/0002_core_entities.py:143:      SELECT RAISE(ABORT, 'append-only: links cannot be deleted');

== [H] gate_models.sh (top 220 lines) ==
#!/usr/bin/env bash
set +e

ROOT="$(git rev-parse --show-toplevel 2>/dev/null)"
if [ -z "$ROOT" ]; then echo "[err] not a git repo"; exit 2; fi
cd "$ROOT" || exit 2

ok()  { echo "[ok] $*"; }
warn(){ echo "[warn] $*"; }
err() { echo "[err] $*"; }

echo "== gate_models: start =="

pushd "$ROOT/apps/api" >/dev/null
export DATABASE_URL="${DATABASE_URL:-sqlite:///./data/app.db}"
python -m alembic -c alembic.ini upgrade head
RC_UP=$?
popd >/dev/null
if [ $RC_UP -ne 0 ]; then
  err "alembic upgrade head failed (rc=$RC_UP)"
  exit 10
fi
ok "alembic upgrade head ok"

python - <<'PY'
from __future__ import annotations
import os
from pathlib import Path
from sqlalchemy import create_engine, text

def repo_root() -> Path:
    return Path(".").resolve()

def resolve_sqlite_file(url: str) -> Path | None:
    if not url.startswith("sqlite:///"):
        return None
    p = url[len("sqlite:///"):]
    if p.startswith("/"):
        return Path(p)
    if len(p) >= 3 and p[1] == ":" and (p[2] == "/" or p[2] == "\\"):
        return Path(p)
    return (repo_root() / p).resolve()

url = os.getenv("DATABASE_URL", "sqlite:///./data/app.db")
connect_args = {}
if url.startswith("sqlite:///"):
    connect_args = {"check_same_thread": False}
    f = resolve_sqlite_file(url)
    if f is not None:
        f.parent.mkdir(parents=True, exist_ok=True)
        url = "sqlite:///" + f.as_posix()

engine = create_engine(url, future=True, connect_args=connect_args)

required_tables = [
    "assets","prompt_packs","runs","reviews","links",
    "projects","series","shots",
]

required_asset_cols = {"id","kind","created_at","deleted_at","project_id","series_id"}

required_triggers = [
    "trg_prompt_packs_no_update","trg_prompt_packs_no_delete",
    "trg_runs_no_update","trg_runs_no_delete",
    "trg_reviews_no_update","trg_reviews_no_delete",
    "trg_links_no_update","trg_links_no_delete",
]

def fetchall(sql: str, **params):
    with engine.connect() as c:
        return list(c.execute(text(sql), params).fetchall())

# tables
rows = fetchall("SELECT name FROM sqlite_master WHERE type='table'")
tables = sorted({r[0] for r in rows})
missing_tables = [t for t in required_tables if t not in tables]
if missing_tables:
    print("[err] missing required tables:", missing_tables)
    raise SystemExit(20)
print("[ok] required tables exist: " + ", ".join(required_tables))

# assets columns + nullable
cols = fetchall("PRAGMA table_info('assets')")
# (cid, name, type, notnull, dflt_value, pk)
asset_info = {r[1]: {"notnull": r[3]} for r in cols}
missing_cols = sorted(list(required_asset_cols - set(asset_info.keys())))
if missing_cols:
    print("[err] assets missing columns:", missing_cols)
    raise SystemExit(21)

if asset_info["project_id"]["notnull"] != 0 or asset_info["series_id"]["notnull"] != 0:
    print("[err] assets.project_id/series_id must be nullable (notnull=0). got:",
          {"project_id_notnull": asset_info["project_id"]["notnull"], "series_id_notnull": asset_info["series_id"]["notnull"]})
    raise SystemExit(22)

print("[ok] assets.deleted_at present")
print("[ok] assets.project_id/series_id nullable (unbound allowed)")

# triggers
trows = fetchall("SELECT name FROM sqlite_master WHERE type='trigger'")
trigs = {r[0] for r in trows}
missing_trigs = [t for t in required_triggers if t not in trigs]
if missing_trigs:
    print("[err] missing append-only triggers:", missing_trigs)
    raise SystemExit(23)
print("[ok] immutability policy enforced (append-only triggers present)")

# nullable columns in optional hierarchy
def check_nullable_col(table: str, col: str):
    rows = fetchall(f"PRAGMA table_info('{table}')")
    info = {r[1]: r[3] for r in rows}
    if col not in info:
        print(f"[err] {table} missing column {col}")
        raise SystemExit(24)
    if info[col] != 0:
        print(f"[err] {table}.{col} must be nullable (notnull=0). got notnull={info[col]}")
        raise SystemExit(25)

check_nullable_col("series", "project_id")
check_nullable_col("shots", "project_id")
check_nullable_col("shots", "series_id")
print("[ok] optional hierarchy nullable columns ok")

print("[ok] gate_models passed")
PY

RC=$?
if [ $RC -ne 0 ]; then
  err "gate_models failed (rc=$RC)"
  exit $RC
fi

ok "gate_models passed"
exit 0

