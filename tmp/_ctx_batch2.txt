== [ctx] repo ==
root=E:/BaiduNetdiskDownload/ai-content-workstation
branch=dev/v1_1-batch1-step020-data_model
head=452af87f262e804ca207590ef3b4e6083651ad3f

----- FILE: apps/api/app/modules/provider_profiles/models.py (first 260 lines) -----
     1	from __future__ import annotations
     2	
     3	from sqlmodel import SQLModel, Field
     4	
     5	
     6	class ProviderProfile(SQLModel, table=True):
     7	    __tablename__ = "provider_profiles"
     8	
     9	    id: str = Field(primary_key=True)
    10	    name: str
    11	    provider_type: str
    12	    config_json: str
    13	    secrets_redaction_policy_json: str
    14	
    15	    # 0|1; at most one row can be 1 (DB partial unique index in migration)
    16	    is_global_default: int = Field(default=0)
    17	
    18	    created_at: str
    19	    updated_at: str

----- FILE: apps/api/app/modules/characters/models.py (first 320 lines) -----
     1	from __future__ import annotations
     2	
     3	from typing import Optional
     4	from sqlmodel import SQLModel, Field
     5	
     6	
     7	# v1.1 lock: draft|confirmed|archived
     8	class Character(SQLModel, table=True):
     9	    __tablename__ = "characters"
    10	
    11	    id: str = Field(primary_key=True)
    12	    name: str
    13	    status: str  # draft|confirmed|archived
    14	    active_ref_set_id: Optional[str] = Field(default=None)
    15	
    16	    created_at: str
    17	    updated_at: str
    18	
    19	
    20	# append-only (enforced by SQLite triggers in migration)
    21	class CharacterRefSet(SQLModel, table=True):
    22	    __tablename__ = "character_ref_sets"
    23	
    24	    id: str = Field(primary_key=True)
    25	    character_id: str = Field(foreign_key="characters.id")
    26	    version: int
    27	    status: str  # draft|confirmed|archived
    28	    min_requirements_snapshot_json: str
    29	    created_at: str

----- FILE: apps/api/app/modules/runs/schemas.py (first 320 lines) -----
     1	from __future__ import annotations
     2	
     3	from typing import Any, Dict, List, Literal, Optional
     4	from pydantic import BaseModel, Field
     5	
     6	RunType = Literal["t2i", "i2i", "t2v", "i2v"]
     7	
     8	
     9	class PromptPackIn(BaseModel):
    10	    # Flexible payload for P0. ProviderAdapter will normalize later.
    11	    kind: Optional[str] = None
    12	    prompt: Optional[str] = None
    13	    negative_prompt: Optional[str] = None
    14	    params: Dict[str, Any] = Field(default_factory=dict)
    15	    references: List[Dict[str, Any]] = Field(default_factory=list)
    16	    extra: Dict[str, Any] = Field(default_factory=dict)
    17	
    18	
    19	class RunCreateIn(BaseModel):
    20	    run_type: RunType = "t2i"
    21	    prompt_pack: PromptPackIn
    22	
    23	
    24	class RunCreateOut(BaseModel):
    25	    run_id: str
    26	    prompt_pack_id: str
    27	    status: str
    28	
    29	
    30	class RunGetOut(BaseModel):
    31	    run_id: str
    32	    prompt_pack_id: str
    33	    status: str
    34	    result_refs: Dict[str, Any] = Field(default_factory=dict)
    35	    created_at: Optional[str] = None

----- FILE: apps/api/app/modules/runs/service.py (first 360 lines) -----
     1	from __future__ import annotations
     2	
     3	import json
     4	import os
     5	import sqlite3
     6	import time
     7	from datetime import datetime, timezone
     8	from typing import Any, Dict, List, Optional, Tuple
     9	
    10	DEFAULT_DATABASE_URL = "sqlite:///./data/app.db"
    11	_CROCKFORD32 = "0123456789ABCDEFGHJKMNPQRSTVWXYZ"
    12	
    13	
    14	def _encode_crockford(value: int, length: int) -> str:
    15	    chars: List[str] = []
    16	    for _ in range(length):
    17	        chars.append(_CROCKFORD32[value & 31])
    18	        value >>= 5
    19	    return "".join(reversed(chars))
    20	
    21	
    22	def new_ulid() -> str:
    23	    # 48-bit time (ms) + 80-bit randomness
    24	    ms = int(time.time() * 1000) & ((1 << 48) - 1)
    25	    rnd = int.from_bytes(os.urandom(10), "big")
    26	    v = (ms << 80) | rnd
    27	    return _encode_crockford(v, 26)
    28	
    29	
    30	def _now_iso() -> str:
    31	    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")
    32	
    33	
    34	def _sqlite_path_from_url(url: str) -> str:
    35	    if not url.startswith("sqlite:"):
    36	        raise ValueError(f"Only sqlite supported for now, got DATABASE_URL={url!r}")
    37	    if url.startswith("sqlite:////"):
    38	        return url[len("sqlite:////") - 1 :]  # keep leading '/'
    39	    if url.startswith("sqlite:///"):
    40	        return url[len("sqlite:///") :]
    41	    if url.startswith("sqlite://"):
    42	        return url[len("sqlite://") :]
    43	    raise ValueError(f"Unrecognized sqlite DATABASE_URL format: {url!r}")
    44	
    45	
    46	def _connect() -> sqlite3.Connection:
    47	    url = os.getenv("DATABASE_URL", DEFAULT_DATABASE_URL)
    48	    path = _sqlite_path_from_url(url)
    49	    conn = sqlite3.connect(path)
    50	    conn.row_factory = sqlite3.Row
    51	    return conn
    52	
    53	
    54	def _table_info(conn: sqlite3.Connection, table: str) -> Dict[str, Dict[str, Any]]:
    55	    out: Dict[str, Dict[str, Any]] = {}
    56	    rows = conn.execute(f"PRAGMA table_info({table});").fetchall()
    57	    for r in rows:
    58	        out[r["name"]] = {
    59	            "type": (r["type"] or "").upper(),
    60	            "notnull": int(r["notnull"]),
    61	            "dflt_value": r["dflt_value"],
    62	            "pk": int(r["pk"]),
    63	        }
    64	    return out
    65	
    66	
    67	def _table_exists(conn: sqlite3.Connection, table: str) -> bool:
    68	    row = conn.execute(
    69	        "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
    70	        (table,),
    71	    ).fetchone()
    72	    return row is not None
    73	
    74	
    75	def _primary_key_name(cols: Dict[str, Dict[str, Any]]) -> Optional[str]:
    76	    for name, info in cols.items():
    77	        if int(info.get("pk", 0)) == 1:
    78	            return name
    79	    if "id" in cols:
    80	        return "id"
    81	    return None
    82	
    83	
    84	def _fill_required(cols: Dict[str, Dict[str, Any]], base: Dict[str, Any]) -> Dict[str, Any]:
    85	    data = dict(base)
    86	    now = _now_iso()
    87	
    88	    pk = _primary_key_name(cols)
    89	    if pk and pk not in data:
    90	        t = (cols.get(pk, {}).get("type") or "").upper()
    91	        # if PK is integer, let DB autoincrement; otherwise generate ULID
    92	        if "INT" not in t:
    93	            data[pk] = new_ulid()
    94	
    95	    # timestamps (only if NOT NULL and no default)
    96	    for k in ("created_at", "updated_at", "submitted_at"):
    97	        if k in cols and cols[k]["notnull"] == 1 and cols[k]["dflt_value"] is None and k not in data:
    98	            data[k] = now
    99	
   100	    # status default (only if required)
   101	    if "status" in cols and cols["status"]["notnull"] == 1 and cols["status"]["dflt_value"] is None and "status" not in data:
   102	        data["status"] = "queued"
   103	
   104	    # final pass: NOT NULL w/o default and still missing
   105	    for name, info in cols.items():
   106	        if info["notnull"] != 1:
   107	            continue
   108	        if info["dflt_value"] is not None:
   109	            continue
   110	        if info["pk"] == 1:
   111	            continue
   112	        if name in data:
   113	            continue
   114	
   115	        t = info["type"]
   116	        if name.endswith("_id"):
   117	            data[name] = new_ulid()
   118	        elif "INT" in t:
   119	            data[name] = 0
   120	        elif "REAL" in t or "FLOA" in t or "DOUB" in t:
   121	            data[name] = 0.0
   122	        else:
   123	            data[name] = ""
   124	
   125	    return data
   126	
   127	
   128	def _insert(conn: sqlite3.Connection, table: str, row: Dict[str, Any]) -> str:
   129	    cols = _table_info(conn, table)
   130	    data = _fill_required(cols, row)
   131	
   132	    keys = [k for k in data.keys() if k in cols]
   133	    keys.sort()
   134	    sql = f"INSERT INTO {table} ({','.join(keys)}) VALUES ({','.join(['?'] * len(keys))});"
   135	    cur = conn.execute(sql, [data[k] for k in keys])
   136	
   137	    pk = _primary_key_name(cols)
   138	    if pk and pk in data:
   139	        return str(data[pk])
   140	
   141	    # fallback: lastrowid for integer PKs
   142	    try:
   143	        return str(cur.lastrowid)
   144	    except Exception:
   145	        return ""
   146	
   147	
   148	def create_run(run_type: str, prompt_pack: Dict[str, Any]) -> Tuple[str, str, str]:
   149	    """
   150	    Append-only: creates PromptPack + Run as two INSERTs.
   151	    Returns (run_id, prompt_pack_id, status)
   152	    """
   153	    conn = _connect()
   154	    try:
   155	        if not _table_exists(conn, "prompt_packs"):
   156	            raise RuntimeError("DB missing table: prompt_packs")
   157	        if not _table_exists(conn, "runs"):
   158	            raise RuntimeError("DB missing table: runs")
   159	
   160	        pp_cols = _table_info(conn, "prompt_packs")
   161	        payload = json.dumps({"run_type": run_type, **(prompt_pack or {})}, ensure_ascii=False)
   162	
   163	        pp_row: Dict[str, Any] = {}
   164	        # put payload into first matching column
   165	        for k in ("payload_json", "payload", "input_json", "input", "meta_json", "meta", "params_json", "params"):
   166	            if k in pp_cols:
   167	                pp_row[k] = payload
   168	                break
   169	        if "type" in pp_cols:
   170	            pp_row["type"] = run_type
   171	        if "kind" in pp_cols:
   172	            pp_row["kind"] = "prompt_pack"
   173	
   174	        prompt_pack_id = _insert(conn, "prompt_packs", pp_row)
   175	
   176	        run_cols = _table_info(conn, "runs")
   177	        run_row: Dict[str, Any] = {}
   178	
   179	        # FK to prompt_pack (best-effort)
   180	        for k in ("prompt_pack_id", "promptpack_id", "prompt_pack_ulid"):
   181	            if k in run_cols:
   182	                run_row[k] = prompt_pack_id
   183	                break
   184	
   185	        if "run_type" in run_cols:
   186	            run_row["run_type"] = run_type
   187	        if "type" in run_cols:
   188	            run_row["type"] = run_type
   189	        if "status" in run_cols:
   190	            run_row["status"] = "queued"
   191	
   192	        rr = json.dumps({"asset_ids": []}, ensure_ascii=False)
   193	        for k in ("result_refs_json", "result_refs", "result_json", "result"):
   194	            if k in run_cols:
   195	                run_row[k] = rr
   196	                break
   197	
   198	        run_id = _insert(conn, "runs", run_row)
   199	
   200	        conn.commit()
   201	        return str(run_id), str(prompt_pack_id), "queued"
   202	    finally:
   203	        conn.close()
   204	
   205	
   206	
   207	
   208	def update_run(
   209	    run_id: str,
   210	    *,
   211	    status: Optional[str] = None,
   212	    result_refs: Optional[Dict[str, Any]] = None,
   213	) -> bool:
   214	    """
   215	    Best-effort UPDATE for runs table (P1 provider execution).
   216	    - Does NOT change API shape.
   217	    - Updates only columns that exist.
   218	    Returns True if an UPDATE was attempted (i.e., had at least one field to set).
   219	    """
   220	    conn = _connect()
   221	    try:
   222	        if not _table_exists(conn, "runs"):
   223	            raise RuntimeError("DB missing table: runs")
   224	
   225	        cols = _table_info(conn, "runs")
   226	        pk = _primary_key_name(cols)
   227	        id_col = pk if pk else ("id" if "id" in cols else ("run_id" if "run_id" in cols else "id"))
   228	
   229	        updates: Dict[str, Any] = {}
   230	
   231	        if status is not None and "status" in cols:
   232	            updates["status"] = status
   233	
   234	        if result_refs is not None:
   235	            rr_json = json.dumps(result_refs, ensure_ascii=False)
   236	            for k in ("result_refs_json", "result_refs", "result_json", "result"):
   237	                if k in cols:
   238	                    updates[k] = rr_json
   239	                    break
   240	
   241	        # timestamps (best effort)
   242	        if "updated_at" in cols:
   243	            updates["updated_at"] = _now_iso()
   244	
   245	        if not updates:
   246	            return False
   247	
   248	        keys = sorted(updates.keys())
   249	        set_sql = ", ".join([f"{k}=?" for k in keys])
   250	        sql = f"UPDATE runs SET {set_sql} WHERE {id_col}=?;"
   251	        conn.execute(sql, [updates[k] for k in keys] + [run_id])
   252	        conn.commit()
   253	        return True
   254	    finally:
   255	        conn.close()
   256	
   257	
   258	def get_run(run_id: str) -> Optional[Dict[str, Any]]:
   259	    conn = _connect()
   260	    try:
   261	        if not _table_exists(conn, "runs"):
   262	            raise RuntimeError("DB missing table: runs")
   263	
   264	        cols = _table_info(conn, "runs")
   265	        pk = _primary_key_name(cols)
   266	        id_col = pk if pk else ("id" if "id" in cols else ("run_id" if "run_id" in cols else "id"))
   267	
   268	        row = conn.execute(f"SELECT * FROM runs WHERE {id_col}=? LIMIT 1;", (run_id,)).fetchone()
   269	        if row is None:
   270	            return None
   271	
   272	        prompt_pack_id = ""
   273	        for k in ("prompt_pack_id", "promptpack_id", "prompt_pack_ulid"):
   274	            if k in row.keys() and row[k] is not None:
   275	                prompt_pack_id = str(row[k])
   276	                break
   277	
   278	        created_at = None
   279	        for k in ("created_at", "submitted_at"):
   280	            if k in row.keys():
   281	                created_at = row[k]
   282	                break
   283	
   284	        status = str(row["status"]) if "status" in row.keys() and row["status"] is not None else ""
   285	
   286	        result_refs: Dict[str, Any] = {}
   287	        for k in ("result_refs_json", "result_refs", "result_json", "result"):
   288	            if k in row.keys() and row[k]:
   289	                try:
   290	                    result_refs = json.loads(row[k])
   291	                except Exception:
   292	                    result_refs = {}
   293	                break
   294	
   295	
   296	        # overlay: latest run_events (append-only transitions)
   297	        ev = _get_latest_run_event(conn, str(row[id_col]))
   298	        if ev:
   299	            if ev.get('status'):
   300	                status = str(ev['status'])
   301	            if isinstance(ev.get('result_refs'), dict):
   302	                result_refs = ev['result_refs']
   303	
   304	        return {
   305	            "run_id": str(row[id_col]),
   306	            "prompt_pack_id": prompt_pack_id,
   307	            "status": status,
   308	            "created_at": str(created_at) if created_at is not None else None,
   309	            "result_refs": result_refs if isinstance(result_refs, dict) else {},
   310	        }
   311	    finally:
   312	        conn.close()
   313	
   314	
   315	def _ensure_run_events_table(conn: sqlite3.Connection) -> None:
   316	    """
   317	    Append-only event log for Run status transitions & results (P1 ProviderAdapter).
   318	    This avoids UPDATE on runs table (runs is append-only).
   319	    """
   320	    conn.execute(
   321	        """
   322	        CREATE TABLE IF NOT EXISTS run_events (
   323	            event_id TEXT PRIMARY KEY,
   324	            run_id TEXT NOT NULL,
   325	            status TEXT NOT NULL,
   326	            result_refs_json TEXT,
   327	            request_id TEXT,
   328	            created_at TEXT NOT NULL
   329	        );
   330	        """
   331	    )
   332	    conn.execute(
   333	        "CREATE INDEX IF NOT EXISTS idx_run_events_run_id_created_at ON run_events(run_id, created_at);"
   334	    )
   335	
   336	
   337	def append_run_event(
   338	    run_id: str,
   339	    *,
   340	    status: str,
   341	    result_refs: Optional[Dict[str, Any]] = None,
   342	    request_id: Optional[str] = None,
   343	) -> str:
   344	    """
   345	    Append-only insert of a run event.
   346	    Returns event_id.
   347	    """
   348	    conn = _connect()
   349	    try:
   350	        _ensure_run_events_table(conn)
   351	        event_id = new_ulid()
   352	        now = _now_iso()
   353	        rr_json = None
   354	        if result_refs is not None:
   355	            rr_json = json.dumps(result_refs, ensure_ascii=False)
   356	        conn.execute(
   357	            "INSERT INTO run_events (event_id, run_id, status, result_refs_json, request_id, created_at) VALUES (?,?,?,?,?,?);",
   358	            (event_id, run_id, status, rr_json, request_id or "", now),
   359	        )
   360	        conn.commit()

----- FILE: apps/api/app/modules/runs/providers/base.py (first 260 lines) -----
     1	from __future__ import annotations
     2	
     3	from dataclasses import dataclass
     4	from typing import Any, Dict, List, Optional, Protocol
     5	
     6	
     7	@dataclass(frozen=True)
     8	class ProviderResult:
     9	    """
    10	    Internal execution result returned by provider implementations.
    11	
    12	    NOTE:
    13	    - status must map to existing Run status strings (we will use: queued/running/succeeded/failed).
    14	    - result_refs must be compatible with RunGetOut.result_refs (list[str]).
    15	    """
    16	    status: str
    17	    result_refs: List[str]
    18	    details: Optional[Dict[str, Any]] = None
    19	
    20	
    21	class ProviderAdapter(Protocol):
    22	    """
    23	    ProviderAdapter is a pluggable execution interface for Runs.
    24	    Keep it minimal in P1; do NOT introduce external infra here.
    25	    """
    26	    name: str
    27	
    28	    def execute(self, *, run_id: str, input: Dict[str, Any], request_id: str) -> ProviderResult:
    29	        ...

----- FILE: apps/api/app/modules/runs/providers/mock_provider.py (first 260 lines) -----
     1	from __future__ import annotations
     2	
     3	import json
     4	import os
     5	import time
     6	from pathlib import Path
     7	from typing import Any, Dict
     8	
     9	from .base import ProviderResult
    10	
    11	
    12	class MockProvider:
    13	    """
    14	    Minimal executable provider for P1:
    15	    - writes a JSON artifact into storage root
    16	    - returns a stable storage ref in result_refs
    17	    """
    18	    name = "mock"
    19	
    20	    def __init__(self, storage_root: str | None = None) -> None:
    21	        self.storage_root = storage_root or os.environ.get("STORAGE_ROOT") or "./data/storage"
    22	
    23	    def execute(self, *, run_id: str, input: Dict[str, Any], request_id: str) -> ProviderResult:
    24	        if input.get('__force_fail__'):
    25	            raise RuntimeError('forced failure')
    26	
    27	        out_dir = Path(self.storage_root) / "runs" / run_id
    28	        out_dir.mkdir(parents=True, exist_ok=True)
    29	
    30	        out_file = out_dir / "result.json"
    31	        payload: Dict[str, Any] = {
    32	            "provider": self.name,
    33	            "run_id": run_id,
    34	            "request_id": request_id,
    35	            "ts": time.time(),
    36	            "input": input,
    37	        }
    38	        out_file.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    39	
    40	        # IMPORTANT: keep ref shape stable and human-readable.
    41	        # If your repo already uses a different ref convention, we'll align in the next step.
    42	        ref = f"storage://runs/{run_id}/result.json"
    43	
    44	        return ProviderResult(status="succeeded", result_refs=[ref], details={"path": str(out_file)})

----- FILE: apps/api/app/modules/assets/schemas.py (first 260 lines) -----
     1	from __future__ import annotations
     2	
     3	from typing import Dict, List, Optional
     4	
     5	from pydantic import BaseModel, Field
     6	
     7	
     8	class PageOut(BaseModel):
     9	    limit: int = Field(..., ge=1)
    10	    offset: int = Field(..., ge=0)
    11	    total: int = Field(..., ge=0)
    12	    has_more: bool
    13	
    14	
    15	class AssetDTO(BaseModel):
    16	    id: str
    17	    type: Optional[str] = None
    18	    created_at: Optional[str] = None
    19	    deleted_at: Optional[str] = None
    20	
    21	    project_id: Optional[str] = None
    22	    series_id: Optional[str] = None
    23	
    24	    storage_path: Optional[str] = None
    25	    mime_type: Optional[str] = None
    26	    width: Optional[int] = None
    27	    height: Optional[int] = None
    28	    duration_ms: Optional[int] = None
    29	
    30	
    31	class AssetListOut(BaseModel):
    32	    items: List[AssetDTO]
    33	    page: PageOut
    34	
    35	
    36	class TraceabilityOut(BaseModel):
    37	    # placeholder; service degrades safely if links schema not present
    38	    links: List[Dict] = Field(default_factory=list)
    39	    related: Dict[str, List[str]] = Field(default_factory=dict)
    40	
    41	
    42	class AssetDetailOut(BaseModel):
    43	    asset: AssetDTO
    44	    traceability: TraceabilityOut
    45	
    46	
    47	class AssetDeleteResponse(BaseModel):
    48	    asset_id: str
    49	    deleted_at: Optional[str] = None
    50	    already_deleted: bool = False
    51	    status: str = "deleted"

----- FILE: apps/api/app/modules/assets/service.py (first 360 lines) -----
     1	from __future__ import annotations
     2	
     3	from fastapi import HTTPException
     4	from datetime import datetime
     5	
     6	import os
     7	import sqlite3
     8	from typing import Any, Dict, List, Optional, Tuple
     9	
    10	DEFAULT_DATABASE_URL = "sqlite:///./data/app.db"
    11	
    12	
    13	def _sqlite_path_from_url(url: str) -> str:
    14	    # Supports:
    15	    # - sqlite:///./data/app.db
    16	    # - sqlite:////absolute/path/app.db
    17	    if not url.startswith("sqlite:"):
    18	        raise ValueError(f"Only sqlite is supported for now, got DATABASE_URL={url!r}")
    19	    if url.startswith("sqlite:////"):
    20	        return url[len("sqlite:////") - 1 :]  # keep leading '/'
    21	    if url.startswith("sqlite:///"):
    22	        return url[len("sqlite:///") :]
    23	    if url.startswith("sqlite://"):
    24	        return url[len("sqlite://") :]
    25	    raise ValueError(f"Unrecognized sqlite DATABASE_URL format: {url!r}")
    26	
    27	
    28	def _connect() -> sqlite3.Connection:
    29	    url = os.getenv("DATABASE_URL", DEFAULT_DATABASE_URL)
    30	    path = _sqlite_path_from_url(url)
    31	    conn = sqlite3.connect(path)
    32	    conn.row_factory = sqlite3.Row
    33	    return conn
    34	
    35	
    36	def _table_exists(conn: sqlite3.Connection, table: str) -> bool:
    37	    row = conn.execute(
    38	        "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
    39	        (table,),
    40	    ).fetchone()
    41	    return row is not None
    42	
    43	
    44	def _columns(conn: sqlite3.Connection, table: str) -> List[str]:
    45	    rows = conn.execute(f"PRAGMA table_info({table})").fetchall()
    46	    return [r["name"] for r in rows]
    47	
    48	
    49	def _order_by_expr(cols: List[str]) -> str:
    50	    if "created_at" in cols:
    51	        return "created_at DESC"
    52	    if "id" in cols:
    53	        return "id DESC"
    54	    return "rowid DESC"
    55	
    56	
    57	def _row_to_asset_dict(row: sqlite3.Row, cols: List[str]) -> Dict[str, Any]:
    58	    # Only fields known by schemas.AssetDTO (avoid unknown keys causing drift)
    59	    keep = [
    60	        "id",
    61	        "type",
    62	        "created_at",
    63	        "deleted_at",
    64	        "project_id",
    65	        "series_id",
    66	        "storage_path",
    67	        "mime_type",
    68	        "width",
    69	        "height",
    70	        "duration_ms",
    71	    ]
    72	    out: Dict[str, Any] = {}
    73	    for k in keep:
    74	        if k in cols:
    75	            out[k] = row[k]
    76	
    77	    # best-effort legacy mappings (if present)
    78	    if "storage_uri" in cols and "storage_path" not in out:
    79	        out["storage_path"] = row["storage_uri"]
    80	    if "path" in cols and "storage_path" not in out:
    81	        out["storage_path"] = row["path"]
    82	
    83	    return out
    84	
    85	
    86	def list_assets(limit: int, offset: int, include_deleted: bool) -> Tuple[List[Dict[str, Any]], int]:
    87	    conn = _connect()
    88	    try:
    89	        if not _table_exists(conn, "assets"):
    90	            raise RuntimeError("DB missing table: assets")
    91	
    92	        cols = _columns(conn, "assets")
    93	
    94	        where = ""
    95	        params: List[Any] = []
    96	        if (not include_deleted) and ("deleted_at" in cols):
    97	            where = "WHERE deleted_at IS NULL"
    98	
    99	        total = conn.execute(f"SELECT COUNT(1) AS c FROM assets {where}", params).fetchone()["c"]
   100	
   101	        order_by = _order_by_expr(cols)
   102	        rows = conn.execute(
   103	            f"SELECT * FROM assets {where} ORDER BY {order_by} LIMIT ? OFFSET ?",
   104	            (*params, limit, offset),
   105	        ).fetchall()
   106	        items = [_row_to_asset_dict(r, cols) for r in rows]
   107	        return items, int(total)
   108	    finally:
   109	        conn.close()
   110	
   111	
   112	def get_asset(asset_id: str) -> Optional[Dict[str, Any]]:
   113	    conn = _connect()
   114	    try:
   115	        if not _table_exists(conn, "assets"):
   116	            raise RuntimeError("DB missing table: assets")
   117	        cols = _columns(conn, "assets")
   118	        row = conn.execute("SELECT * FROM assets WHERE id = ? LIMIT 1", (asset_id,)).fetchone()
   119	        if row is None:
   120	            return None
   121	        return _row_to_asset_dict(row, cols)
   122	    finally:
   123	        conn.close()
   124	
   125	
   126	def traceability_for_asset(asset_id: str) -> Dict[str, Any]:
   127	    """
   128	    Minimal traceability summary:
   129	    - links: up to 200 rows from links table that reference this asset
   130	    - related: id map (best-effort) extracted from link endpoints
   131	    Degrades safely if links table/schema not present.
   132	    """
   133	    conn = _connect()
   134	    try:
   135	        if not _table_exists(conn, "links"):
   136	            return {"links": [], "related": {}}
   137	
   138	        cols = _columns(conn, "links")
   139	        # Check for actual column names used in the database (src_type/dst_type or source_type/target_type)
   140	        has_src_dst = {"src_type", "src_id", "dst_type", "dst_id"}.issubset(set(cols))
   141	        has_source_target = {"source_type", "source_id", "target_type", "target_id"}.issubset(set(cols))
   142	        
   143	        if not has_src_dst and not has_source_target:
   144	            return {"links": [], "related": {}}
   145	
   146	        # Use the correct column names based on what's in the database
   147	        if has_src_dst:
   148	            src_col = "src_type"
   149	            src_id_col = "src_id"
   150	            dst_col = "dst_type"
   151	            dst_id_col = "dst_id"
   152	            rel_col = "rel"
   153	        else:
   154	            src_col = "source_type"
   155	            src_id_col = "source_id"
   156	            dst_col = "target_type"
   157	            dst_id_col = "target_id"
   158	            rel_col = "relation"
   159	
   160	        rows = conn.execute(
   161	            f"""
   162	            SELECT * FROM links
   163	            WHERE ({src_col} = ? AND {src_id_col} = ?)
   164	               OR ({dst_col} = ? AND {dst_id_col} = ?)
   165	            ORDER BY rowid DESC
   166	            LIMIT 200
   167	            """,
   168	            ("asset", asset_id, "asset", asset_id),
   169	        ).fetchall()
   170	
   171	        links: List[Dict[str, Any]] = []
   172	        related: Dict[str, List[str]] = {}
   173	
   174	        def add_related(t: str, i: str) -> None:
   175	            key = f"{t}_ids"
   176	            arr = related.setdefault(key, [])
   177	            if i not in arr:
   178	                arr.append(i)
   179	
   180	        for r in rows:
   181	            d: Dict[str, Any] = {}
   182	            # Map to output format (always use source/target in output for consistency)
   183	            for k in ["id", "created_at"]:
   184	                if k in cols:
   185	                    d[k] = r[k]
   186	            
   187	            # Map src/dst to source/target in output
   188	            if has_src_dst:
   189	                d["source_type"] = r[src_col]
   190	                d["source_id"] = r[src_id_col]
   191	                d["target_type"] = r[dst_col]
   192	                d["target_id"] = r[dst_id_col]
   193	                d["relation"] = r[rel_col]
   194	            else:
   195	                d["source_type"] = r[src_col]
   196	                d["source_id"] = r[src_id_col]
   197	                d["target_type"] = r[dst_col]
   198	                d["target_id"] = r[dst_id_col]
   199	                d["relation"] = r[rel_col]
   200	            
   201	            links.append(d)
   202	
   203	            # Extract values for related items
   204	            st = r[src_col]
   205	            sid = r[src_id_col]
   206	            tt = r[dst_col]
   207	            tid = r[dst_id_col]
   208	            if st and sid:
   209	                add_related(st, sid)
   210	            if tt and tid:
   211	                add_related(tt, tid)
   212	
   213	        return {"links": links, "related": related}
   214	    finally:
   215	        conn.close()
   216	
   217	def soft_delete_asset(asset_id: str, request_id: Optional[str] = None) -> dict:
   218	    """
   219	    Soft delete (idempotent):
   220	    - sets assets.deleted_at = <UTC ISO8601 'Z'>
   221	    - MUST NOT hard-delete the row
   222	    - repeat calls return success with already_deleted=true
   223	    """
   224	    conn = _connect()
   225	    try:
   226	        if not _table_exists(conn, "assets"):
   227	            raise RuntimeError("DB missing table: assets")
   228	
   229	        cols = _columns(conn, "assets")
   230	
   231	        # Pick id column defensively
   232	        if "id" in cols:
   233	            id_col = "id"
   234	        elif "asset_id" in cols:
   235	            id_col = "asset_id"
   236	        else:
   237	            raise RuntimeError("assets table missing id column (id/asset_id)")
   238	
   239	        if "deleted_at" not in cols:
   240	            raise RuntimeError("assets table missing deleted_at (soft delete invariant broken)")
   241	
   242	        row = conn.execute(
   243	            f"SELECT deleted_at FROM assets WHERE {id_col} = ? LIMIT 1",
   244	            (asset_id,),
   245	        ).fetchone()
   246	
   247	        if row is None:
   248	            raise HTTPException(status_code=404, detail=f"Asset not found: {asset_id}")
   249	
   250	        existing = row["deleted_at"]
   251	        already = existing is not None
   252	
   253	        if not already:
   254	            ts = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
   255	            conn.execute(
   256	                f"UPDATE assets SET deleted_at = ? WHERE {id_col} = ?",
   257	                (ts, asset_id),
   258	            )
   259	            conn.commit()
   260	            deleted_at = ts
   261	        else:
   262	            deleted_at = existing
   263	
   264	        return {
   265	            "asset_id": asset_id,
   266	            "deleted_at": deleted_at,
   267	            "already_deleted": bool(already),
   268	            "status": "deleted",
   269	        }
   270	    finally:
   271	        try:
   272	            conn.close()
   273	        except Exception:
   274	            pass

== [ctx] done: wrote tmp/_ctx_batch2.txt ==
